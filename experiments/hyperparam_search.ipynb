{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralprophet import NeuralProphet, set_log_level\n",
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from optuna.samplers import RandomSampler\n",
    "from optuna.pruners import MedianPruner\n",
    "import joblib\n",
    "from msse import msse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Poor data aggregation\n",
    "\n",
    "datalist = []\n",
    "\n",
    "file_list = [\n",
    "    \"year_2023.json\",\n",
    "    \"year_2024.json\",\n",
    "    \"year_2025.json\"\n",
    "]\n",
    "\n",
    "for file in file_list:\n",
    "    with open(\"data/\" + file, encoding=\"utf-8\") as f:\n",
    "        converted = pd.DataFrame(json.load(f)[\"data\"])\n",
    "\n",
    "        # My 2024 file is different from all the others.\n",
    "        if (\"2024\" in file):\n",
    "            converted = converted.drop([\"main_load\", \"down\"], axis=1)\n",
    "\n",
    "        datalist.append(converted)\n",
    "\n",
    "df = pd.concat(datalist)\n",
    "\n",
    "df['date'] = pd.to_datetime(df['date'], format=\"%Y/%m/%d %H:%M\")\n",
    "df = df.set_index(\"date\")\n",
    "\n",
    "df.sort_values('date')\n",
    "df = df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bring the data to the form that is needed in NeuralProphet.\n",
    "\n",
    "df_gpu = df.rename(columns={\"date\":\"ds\", \"gpu_load\": 'y'}).copy()\n",
    "\n",
    "df_gpu = df_gpu.drop(\"cpu_load\", axis=1)\n",
    "\n",
    "# Exclude anomalies.\n",
    "df_gpu.loc[df_gpu[\"y\"] < 10.58, \"y\"] = pd.NA\n",
    "\n",
    "df_gpu = df_gpu.drop_duplicates(subset=\"ds\", keep=\"first\")\n",
    "\n",
    "# Remove the gaps in the measurements.\n",
    "df_gpu = df_gpu.set_index('ds').asfreq('15min')\n",
    "df_gpu['y'] = df_gpu['y'].interpolate(method='time')\n",
    "df_gpu = df_gpu.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anyway, this doesn't work...\n",
    "\n",
    "set_log_level(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The segments along which the dataset is divided into \n",
    "validation and training samples. The last cut is not \n",
    "the last measurement in order to have a test sample.\n",
    "\"\"\"\n",
    "\n",
    "validation_cutts = [\n",
    "    pd.to_datetime(\"2025-01-25\"),\n",
    "    pd.to_datetime(\"2025-02-01\"),\n",
    "    pd.to_datetime(\"2025-02-08\"),\n",
    "    pd.to_datetime(\"2025-02-15\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The number of predictions that TaskShift model will make\n",
    "\n",
    "TOTAL_FORECASTS = 192"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Hyperparameters are selected as follows: \n",
    "The following happens iteratively for each model: \n",
    "the sample is divided according to the ith section. \n",
    "The first part is a training sample, the following \n",
    "TOTAL_FORECASTS are a validation sample. Then the MSSE\n",
    "is measured on the validation sample and saved. This \n",
    "happens in all sections. Then all the MSSES are averaged \n",
    "and this is the final one.\n",
    "\"\"\"\n",
    "\n",
    "def testing_pararms(df, cut_dates, n_lags, learning_rate=None, epochs=None, batch_size=None, seasonality_reg=0, n_changepoints=10, trend_reg=None):\n",
    "    msse_list = []\n",
    "    for cutoff in cut_dates:\n",
    "        df_train = df[df[\"ds\"] < cutoff].copy()\n",
    "        df_test = df[(df[\"ds\"] >= cutoff) & (df[\"ds\"] < cutoff + pd.Timedelta(days=2))].copy()\n",
    "\n",
    "        model = NeuralProphet(n_lags=n_lags,\n",
    "                            n_forecasts=TOTAL_FORECASTS,\n",
    "                            epochs=epochs,\n",
    "                            learning_rate=learning_rate,\n",
    "                            batch_size=batch_size,\n",
    "                            seasonality_reg=seasonality_reg,\n",
    "                            n_changepoints=n_changepoints,\n",
    "                            trend_reg=trend_reg)\n",
    "\n",
    "        model = model.add_country_holidays(\"RU\")\n",
    "\n",
    "        _ = model.fit(df_train)\n",
    "\n",
    "        df_future = model.make_future_dataframe(df_train, periods=TOTAL_FORECASTS)\n",
    "        forecast = model.predict(df_future)\n",
    "\n",
    "        forecasts_list = model.get_latest_forecast(forecast)[\"origin-0\"].astype(float)\n",
    "\n",
    "        if len(forecasts_list) != df_test.shape[0]:\n",
    "            print(\"Не сходятся количество значений\")\n",
    "\n",
    "        msse_list.append(msse(forecasts_list, df_test[\"y\"]))\n",
    "\n",
    "    return np.mean(msse_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    n_lags = trial.suggest_int(\"n_lags\", 148, 192*3)\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-4, 10)\n",
    "    epochs = trial.suggest_int(\"epochs\", 5, 500)\n",
    "    batch_size = trial.suggest_int(\"batch_size\", 8, 1024)\n",
    "    seasonality_reg = trial.suggest_float(\"seasonality_reg\", 0.1, 10)\n",
    "    n_changepoints = trial.suggest_int(\"n_changepoints\", 1, 15)\n",
    "    trend_reg = trial.suggest_float(\"seasonality_reg\", 0.01, 50)\n",
    "\n",
    "    msse = testing_pararms(df_gpu,\n",
    "                           validation_cutts,\n",
    "                           n_lags=n_lags,\n",
    "                           learning_rate=learning_rate,\n",
    "                           epochs=epochs,\n",
    "                           batch_size=batch_size,\n",
    "                           seasonality_reg=seasonality_reg,\n",
    "                           n_changepoints=n_changepoints,\n",
    "                           trend_reg=trend_reg)\n",
    "\n",
    "    return msse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=60)\n",
    "\n",
    "joblib.dump(study, \"study.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
